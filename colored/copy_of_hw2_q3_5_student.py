# -*- coding: utf-8 -*-
"""Copy of HW2_Q3-5_Student.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kuDCHhACvlJjwk8vwp0L3DRFpaTl3y47

# Homework 2: Imitation Learning

Welcome to the coding portion of Homework 2! This week's problem set focuses on implementing imitation learning/behavioral cloning. The goal is that by the end of the assignment, you will know how to implement behavioral cloning and evaluate the cloned policy. As an added bonus, you will get to train an agent to play Flappy Bird!  

<center>
<img width="300px" src="https://drive.google.com/uc?id=1O4dAX_rN62fjsBRDjaYpz01ZcQ4aVP6A">
</center>

**Notes:**

* **You should be running this on Google Colab!**
* **Be sure to upload the dataset files provided with this .ipynb file using the folder tab on the left, so that you can have access to them.**
"""

# Commented out IPython magic to ensure Python compatibility.
# IMPORTANT: Always run this cell before anything else to ensure that you are able to access the Flappy Bird environment.
from IPython.display import clear_output

# %pip install git+https://github.com/kchua/flappy-bird-gym.git
clear_output()
import flappy_bird_gym
from matplotlib import rc
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import numpy as np

rc('animation', html='jshtml')

def animate_images(img_lst):
  fig, ax = plt.subplots()
  ax.set_axis_off()
  ims = []
  for i, image in enumerate(img_lst):
    im = ax.imshow(image.swapaxes(0, 1), animated=True)
    if i == 0:
      ax.imshow(image.swapaxes(0, 1))
    ims.append([im])
  ani = animation.ArtistAnimation(fig, ims, interval=34)
  return ani

import torch
from torch import nn
from torch.distributions import Distribution

"""## Problem 1: Flap Like How Flappy Sr. Taught You! (AKA Implementing Behavioral Cloning)

Growing up, Flappy Jr. has always dreamed of being as graceful as his mentor, Flappy Sr., in navigating the randomly generated pipes of the world. However, one day, his mentor simply disappeared on a daily pipe dodging adventure -- eyewitness reports say that his mentor is still pipe dodging and has not reset since the day he left.

One day, as he was going about his day, he found a secret cabinet in his mentor's house containing notes of all of his pipe dodging exploits. Flappy Jr. thought to himself, "This is my chance! If I copy whatever he did in these notes, I will be as good as him."

In the following problems, we will help Flappy Jr. by implementing behavioral cloning to make use of Flappy Sr.'s notes.

### (a) Policy Evaluation

Before we get started with any training, let us make sure that we can evaluate whatever policy we get at the end. Recall that in reinforcement learning, the primary way in which we compare policies is by comparing their _returns_. The (infinite discounted) return of a policy $\pi$, denoted $R(\pi)$, is defined as

$$R(\pi) := \mathbb{E}\left[\sum_{t = 0}^\infty \gamma^tr(s_t, a_t)\right],$$

where $s_0 \sim p_0(s)$, and for all $t$, $a_t \sim \pi(s_t)$ and $s_{t + 1} \sim p(s_{t + 1} \ | \ s_t, a_t)$, and $\gamma$ is the discount factor.

**In the following code box, write a function `evaluate_policy`, which given an environment `env`, a callable `policy` which returns an action distribution given a state, and discount factor `discount`, returns a single-sample estimate of $R(\pi)$ as defined above.** `env` is assumed to follow the \*old Gym API, and the action distribution returned by `policy` is a subclass of `torch.distributions.Distribution`. You can assume that the rollout eventually terminates.

\*In newer versions of Gym/Gymnasium, `reset` and `step` are assumed to return `(initial_ob, info_dict)` and `(next_ob, reward, terminated, truncated, info)`'. However, the environment we will be working with for this assignment instead returns `initial_ob` and `(next_ob, reward, done, info)`, following the old API.

(An earlier version incorrectly had `init` instead of `reset` in the comment above, this has been fixed.)
"""

def evaluate_policy(env, policy, discount) -> float:
  """Returns a single-sample estimate of a policy's return.

  Args:
    env: OpenAI gym environment following old API.
    policy: Callable representing a policy.

  Returns:
    Single-sample estimate of return.
  """
  ### YOUR CODE HERE!
  pass
  state = env.reset()
  done = False
  total_reward = 0.0
  t = 0

  while not done:
      state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
      action_dist = policy(state_tensor)
      action = action_dist.sample()
      next_state, reward, done, _ = env.step(action.item())

      total_reward += reward * (discount ** t)
      state = next_state
      t += 1

  return total_reward

"""**As a test, use the function you defined in the block above to evaluate a policy which acts uniformly at random within the Flappy Bird environment over 50 independent rollouts. Use a discount of $0.999$.**

(Note that the discount factor above was changed from $0.99$ in an earlier version; we will not be grading you based on which value you use, but $0.999$ will make it easier to compare different algorithms in this homework).
"""

env = flappy_bird_gym.make("FlappyBird-v0")
### YOUR CODE HERE!
pass
# define a random policy class
class RandomPolicy:
    def __call__(self, state):
      # two actions: flap or not flap
        return torch.distributions.Categorical(logits=torch.tensor([0.5, 0.5]))

discount = 0.999
policy = RandomPolicy()
returns = [evaluate_policy(env, policy, discount) for _ in range(50)]

mean = np.mean(returns)
std = np.std(returns)
print(f"Mean: {mean}\nStd: {std}")

"""### (b) Defining a Policy

**In the code cell below, create a subclass of ``torch.nn.Module`` to represent a policy over a finite set of actions.** Remember, a policy outputs an instance of `torch.distributions.Distribution` (can be one of its subclasses), and the policy must be able to represent any distribution over the finite set of actions! **We have defined the skeleton for you below; do not modify the inputs to the functions.**

Note: Feel free to define the architecture however you like; _all we need from you is that you properly initialize the module, and that the forward method is consistent with our definition of a policy._
"""

class DiscretePolicy(nn.Module):
  def __init__(self, input_dim, n_actions):
    """Initializes a policy over the action set {0, 1, ..., n_actions-1}.

    Args:
      input_dim: Observation dimensionality.
      n_actions: Number of actions in environment.
    """
    super().__init__()
    ### YOUR CODE HERE!
    pass
    # two-layer neural network
    self.linear1 = nn.Linear(input_dim,16)
    self.linear2 = nn.Linear(16, n_actions)

  def forward(self, ob) -> Distribution:
    """Returns a distribution over this policy's action set.
    """
    ### YOUR CODE HERE!
    pass
    x = nn.functional.relu(self.linear1(ob))
    x = self.linear2(x)
    return torch.distributions.Categorical(logits=x)

"""### (c) Setting Up Behavioral Cloning

For this problem, we will be defining a standard training loop to perform behavioral cloning. **In the following code cell, write a function which takes in:**

* **a policy `policy` of class `DiscretePolicy` to be trained,**
* **a dataset `dataset` for imitation learning,**
* **number of training steps `n_steps`,**
* **and batch size `batch_size`,**

**and returns the resulting policy after performing training according to the given parameters. Please clearly specify the format in which you expect the dataset to take within the docstring.**


"""

def train_policy_by_bc(policy, dataset, n_steps, batch_size) -> DiscretePolicy:
  """Trains the provided policy by behavioral cloning, by taking n_steps training steps with the
  provided optimizer. During training, training batches of size batch_size are sampled from the dataset
  to compute the loss.

  Args:
    policy: policy of class DiscretePolicy.
    dataset: The dataset, represented as TensorDataset(observations, actions), where observations
              and actions are both tensors from the original dataset
    n_steps: Number of training steps to take.
    batch_size: Size of the sampled batch for each training step.

  Returns:
    A policy trained according to the parameters above.
  """
  ### YOUR CODE HERE!
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  optimizer = torch.optim.Adam(policy.parameters(), lr=0.001)
  criterion = nn.CrossEntropyLoss()

  train_loader = torch.utils.data.DataLoader(
      dataset= dataset,
      batch_size=batch_size,
      shuffle=True
  )

  policy.train()
  for step in range(n_steps):
      for batch_states, batch_actions in train_loader:
          batch_states, batch_actions = batch_states.to(device), batch_actions.to(device)
          optimizer.zero_grad()
          action_dist = policy(batch_states)
          logits = action_dist.logits

          loss = criterion(logits, batch_actions)

          loss.backward()
          optimizer.step()

  return policy

"""### (d) Behavioral Cloning and Evaluation

The time has come for Flappy Jr. to learn from Flappy Sr.'s notes! **Make use of the provided dataset `flappy_sr_notes.mat` and obtain a policy for Flappy Jr. via behavioral cloning. Upon training, evaluate the policy using a discount of 0.99, and verify that it performs much better than selecting actions uniformly at random.** Remember to upload the provided dataset using the folder tab on the left to be able to access it!

Hint 1: You can use `scipy.io.loadmat` to load `.mat` files. Note that uploading may take some time, and `loadmat()` will error out while this process is incomplete; you can check the progress of the upload at the bottom of the folder tab to the left.  
Hint 2: Upon uploading the file, you can `right-click > Copy path` to get the path to the file within the Colab server.  
Hint 3: `flappy_sr_notes.mat` contains a record of Flappy Sr.'s whole life: every one of his rollouts is contained within the dataset in sequential order. The keys available in the dataset are `observations`, `actions`, `rewards`, `next_observations`, and `is_rollout_start` (indicates whether each index is the start of a new rollout). You may not need all keys for behavioral cloning!
"""

### YOUR CODE HERE
pass
import scipy.io
import torch
import numpy as np
from torch.utils.data import DataLoader, TensorDataset

data = scipy.io.loadmat('/content/flappy_sr_notes.mat')
observations = torch.tensor(data['observations'], dtype=torch.float32)
actions = torch.tensor(data['actions'].flatten(), dtype=torch.long)
dataset = TensorDataset(observations, actions)

# define policy
input_dim = observations.shape[1] # number of features
n_actions = len(torch.unique(actions)) # number of actions
policy = DiscretePolicy(input_dim, n_actions)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
policy.to(device)

trained_policy = train_policy_by_bc(
    policy=policy,
    dataset=dataset,
    n_steps=50,
    batch_size=64
)

env = flappy_bird_gym.make("FlappyBird-v0")

# define a callable policy to use for evaluation
class TrainedPolicy:
    def __init__(self, policy):
        self.policy = policy
    def __call__(self, state):
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        return self.policy(state_tensor)

# evaluate trained policy
discount = 0.999
trained_returns = [evaluate_policy(env, TrainedPolicy(trained_policy), discount) for _ in range(50)]
mean_trained_return = np.mean(trained_returns)

# evaluate random policy
random_returns = [evaluate_policy(env, RandomPolicy(), discount) for _ in range(50)]
mean_random_return = np.mean(random_returns)

print(f"Mean return (Trained policy): {mean_trained_return}")
print(f"Mean return (Random policy): {mean_random_return}")

"""### (e) Optional (but Fun!): Visualizing Your Learned Policy Within the Game

Want to see Flappy Jr. majestically soar through the skies? We sure do! We have written for you a convenience function for visualizing a sequence of images as an animation right here within the Colab environment. **All you need to do is to copy your sampling code here from Problem 1(a) and slightly modify it to instead return a sequence of rendered images of every timestep from a rollout.** The generated animation will be saved in the folder tab to the left where you uploaded the dataset.

Hint: At any point, you can call `render(mode="rgb_array")` on a `gym` environment to obtain an RGB image of the current state.
"""

def rollout_and_render(env, policy) -> list[np.ndarray]:
  """Returns a rendering of a single rollout under the provided policy.

  Args:
    env: OpenAI gym environment following old API.
    policy: Callable representing a policy.

  Returns:
    A list of RGB images (as numpy arrays) from a single rollout.
  """
  ### YOUR CODE HERE!

  img_lst = []  # List to store frames

  state = env.reset()
  done = False

  while not done:
      # Capture frame before taking action
      img_lst.append(env.render(mode="rgb_array"))

      # Get action from policy
      state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
      action_dist = policy(state_tensor)
      action = action_dist.sample().item()

      # Step in the environment
      state, _, done, _ = env.step(action)

  return img_lst


img_lst = rollout_and_render(env, TrainedPolicy(trained_policy))    # Assign list of images here

### DO NOT MODIFY ANYTHING BELOW THIS POINT
ani = animate_images(img_lst)
FFwriter = animation.FFMpegWriter(fps=30)
ani.save('animation.mp4', writer=FFwriter)

"""## Problem 2: Floppy the Sloppy Ruins (?) the Day (AKA An Introduction to Filtered Behavioral Cloning)

Flappy Jr. happily went to sleep, satisfied that he has had such a succesful day of flying through the skies imitating Flappy Sr. Alas, he is woken up by some strange noise in the middle of the night, and immediately realizes that someone has run off with their learned model weights. Calmly, Flappy Jr. tells himself that Flappy Sr.'s notes are just in the next room, and that they can always do behavioral cloning again. Flappy Jr. walks into the next room and is horrified to see that the notebook being scribbled over by his nemesis, Floppy the Sloppy. Upon being discovered, Floppy the Sloppy flies off awkwardly flailing through the skies, laughing while they continue their awkward motion one could hardly consider as flying.

### (a): What is Left???

Unfortunately, Floppy the Sloppy has uncharacteristically good penmanship, and his cannot be distinguished from that of Flappy Sr. As a result, we now only have a vandalized dataset `vandalized_notes.mat` consisting of the combined trajectories of Floppy the Sloppy and Flappy Sr. which are indistinguishable from each other at first glance. **For this problem, try applying behavioral cloning to train a new policy as you have done before, only this time using `vandalized-notes.mat.** How does the performance compare to your policy from Problem 1(d)?
"""

### YOUR CODE HERE!
data = scipy.io.loadmat('/content/vandalized_notes.mat')
observations = torch.tensor(data['observations'], dtype=torch.float32)
actions = torch.tensor(data['actions'].flatten(), dtype=torch.long)
dataset = TensorDataset(observations, actions)

# define policy
input_dim = observations.shape[1]  # number of features
n_actions = len(torch.unique(actions))  # number of unique actions
policy = DiscretePolicy(input_dim, n_actions)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

policy.to(device)

# train policy
trained_policy = train_policy_by_bc(
    policy=policy,
    dataset=dataset,
    n_steps=50,
    batch_size=64
)

env = flappy_bird_gym.make("FlappyBird-v0")

# define a callable policy to evaluate the training
class TrainedPolicy:
    def __init__(self, policy):
        self.policy = policy
    def __call__(self, state):
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        return self.policy(state_tensor)

# evaluate trained policy
discount = 0.999
trained_returns = [evaluate_policy(env, TrainedPolicy(trained_policy), discount) for _ in range(50)]
mean_trained_return = np.mean(trained_returns)

# evaluate random policy
random_returns = [evaluate_policy(env, RandomPolicy(), discount) for _ in range(50)]
mean_random_return = np.mean(random_returns)

print(f"Mean return (Trained policy): {mean_trained_return}")
print(f"Mean return (Random policy): {mean_random_return}")

"""**Short Answer: What does this experiment tell you about running behavioral cloning on noisy/low-quality datasets? Intuitively, why does this happen?**

Hint: Think about what the BC loss is optimizing.

⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠
This experiment tells me that running behavioral cloning on noisy/low-quality datasets may be a bad idea since model is trying to mimic the low-quality dataset that may have chosen bad actions for each state and not have maximized reward. Compared to the mean return for the trained policy with flappy senior's notes of around 404, the mean return for the trained policy for the vandalized notes is only around 183. Hence, we should be careful running behavioral cloning on low-quality datasets and try to only use datasets that have minimum noise and high quality to maximize reward.

### (b): Array of Hope???

While deep in thought, Flappy Jr. had a sudden epiphany; he has access to reward data! He can then weigh the importance of imitating a particular example by looking at the quality of the outcome from performing a particular action.

This strategy, where one reweighs BC examples by some predefined notion of quality, is generally referred to as **filtered behavioral cloning**. More formally, assume that we have access to a BC dataset $\{(s_1, a_1), \dots, (s_N, a_N)\}$. Furthermore, assume that we have pre-defined weights $w_1, \dots, w_N$ for each of the $N$ examples. Then, we can consider a _reweighed BC loss_

$$L_w(\pi) = \frac{1}{N}\sum_{i = 1}^{N}w_iL(\pi(s_i), a_i),$$

where $L$ is the standard per-example BC loss (e.g. cross-entropy loss for discrete actions). If the weights represent a notion of quality, one can interpret the loss above as performing filtering to ensure that only high quality examples are being imitated.

**For this problem, you will fill in the skeleton below to implement a training loop based on filtered BC. Firstly, implement the reweighed BC loss given above. Secondly, modify the BC loop you implemented before to make use of this new loss function.**
"""

class ReweighedBCLoss(nn.Module):
  def __init__(self):
    super().__init__()
    # YOUR CODE HERE!
    self.criterion = nn.CrossEntropyLoss()

  def forward(self, batch_predictions, batch_targets, batch_weights) -> torch.Tensor:
    # YOUR CODE HERE!
    indiv_loss = self.criterion(batch_predictions, batch_targets)
    return torch.mean(batch_weights * indiv_loss)

def train_policy_by_filtered_bc(policy, dataset, weights, n_steps, batch_size) -> DiscretePolicy:
  """Trains the provided policy by filtered behavioral cloning, by taking n_steps training steps with the
  provided optimizer with the provided weights. During training, training batches of size batch_size are sampled from the dataset
  to compute the loss.

  Args:
    policy: policy of class DiscretePolicy.
    dataset: The dataset, represented as TensorDataset(observations, actions), where observations
              and actions are both tensors from the original dataset
    weights: Weights used in the filtered BC loss.
    optimizer: An instance of torch.optim.Optimizer.
    n_steps: Number of training steps to take.
    batch_size: Size of the sampled batch for each training step.

  Returns:
    A policy trained according to the parameters above.
  """
  ### YOUR CODE HERE!
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  optimizer = torch.optim.Adam(policy.parameters(), lr=0.001)
  criterion = ReweighedBCLoss()

  # isolate states and actions from dataset to combine with weights in
  # TensorDataset for DataLoader
  states = torch.stack([s for s, _ in dataset]).to(torch.float32)
  actions = torch.tensor([a for _, a in dataset], dtype=torch.long)
  weights = torch.tensor(weights, dtype=torch.float32)

  train_loader = torch.utils.data.DataLoader(
      dataset= TensorDataset(states, actions, weights),
      batch_size=batch_size,
      shuffle=True
  )

  policy.train()
  for step in range(n_steps):
      for batch_states, batch_actions, batch_weights in train_loader:
          batch_states, batch_actions = batch_states.to(device), batch_actions.to(device)
          optimizer.zero_grad()
          action_dist = policy(batch_states)
          logits = action_dist.logits

          loss = criterion(logits, batch_actions, batch_weights)

          loss.backward()
          optimizer.step()

  return policy

"""### (c) Filtering Strategy I: Trajectory-Level Reweighing???

For this problem, we will try a simple strategy: all of the $(s, a)$ pairs obtained from one trajectory will have the same weight, determined by a function of the trajectory return. To formally define this weighing scheme, let $\tau_1, \tau_2, \dots, \tau_M$ denote the trajectories found in the dataset. Then, for any $(s, a)$ contained in $\tau_i$, the corresponding weight $w$ is given by

$$w = \text{Softmax}_i\left[\frac{1}{\alpha}R(\tau_1)\ , \frac{1}{\alpha}R(\tau_2)\ , \dots\ , \frac{1}{\alpha}R(\tau_m)\right],$$

where $R(\tau_i)$ is the discounted return of trajectory $i$ and $\alpha > 0$ is a hyperparameter referred to as the _temperature_.

**Implement the function which computes the weights as defined above in the cell below, and performed filtered behavioral cloning with the computed weights.** Feel free to tune the temperature as necessary. How does the trained policy compare to that of the earlier policies in Problems 1(d) and 2(a)?

Hint: Remember that the dataset includes an array `is_rollout_start` indicating whether each datapoint is the start of a new trajectory.
"""

from scipy.special import softmax

def trajectory_level_return_weights(dataset, temp, discount) -> np.ndarray:
  """Computes an array of weights for each point in the provided dataset according to the trajectory-level reweighing scheme.

  Args:
    dataset: Input dataset.
    temp: Temperature used in softmax.
    discount: Discount used to compute return.

  Returns:
    An array of weights for each BC datapoint in the dataset.
  """
  ### YOUR CODE HERE!
  rewards = data['rewards']
  is_rollout_start = data['is_rollout_start']

  trajectory_starts = np.where(is_rollout_start)[0]
  trajectory_starts = np.append(trajectory_starts, len(rewards))

  # compute returns for each trajectory
  trajectory_returns = []
  for i in range(len(trajectory_starts) - 1):
      start = trajectory_starts[i]
      end = trajectory_starts[i + 1]

      total_return = 0
      for t in range(end - start):
          total_return += rewards[start + t] * (discount ** t)

      trajectory_returns.append(total_return)

  trajectory_weights = softmax(np.array(trajectory_returns) / temp)

  # map back the weights back to each timestep
  for i in range(len(trajectory_starts) - 1):
      start = trajectory_starts[i]
      end = trajectory_starts[i + 1]

      for j in range(start, end):
          weights[j] = trajectory_weights[i]

  return weights

### RUN FILTERED BC HERE!
temperature = 0.5
discount = 0.999

weights = trajectory_level_return_weights(dataset, temperature, discount)
input_dim = observations.shape[1] # number of features
n_actions = len(torch.unique(actions)) # number of actions
policy = DiscretePolicy(input_dim, n_actions)

# train policy
trained_policy = train_policy_by_filtered_bc(
    policy=policy,
    dataset=dataset,
    weights=weights,
    n_steps=50,
    batch_size=64
)

env = flappy_bird_gym.make("FlappyBird-v0")

# define a callable policy for evaluating the training
class TrainedPolicy:
    def __init__(self, policy):
        self.policy = policy
    def __call__(self, state):
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        return self.policy(state_tensor)

# evaluate trained policy
discount = 0.999
trained_returns = [evaluate_policy(env, TrainedPolicy(trained_policy), discount) for _ in range(50)]
mean_trained_return = np.mean(trained_returns)

# evaluate random policy
random_returns = [evaluate_policy(env, RandomPolicy(), discount) for _ in range(50)]
mean_random_return = np.mean(random_returns)

print(f"Mean return (Trained policy): {mean_trained_return}")
print(f"Mean return (Random policy): {mean_random_return}")

"""**Short Answer: Why does the choice of weighing function work here? How does it affect what the behavioral cloning loss is doing?**

⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠
Using softmax on the rewards for each trajectory allows us to prioritize high return trajectories and suppress low return trajectories. This allows the behavioral cloning loss to imitate expert behavior/behavior that achieves high rewards. This allows us to better filter out the noisy/low-quality trajectories and have them impact the training process less, thus reducing overfitting to noisy or poorly performed demonstrations. The mean return is slightly higher now, at around 192.

**Short Answer: What is the effect of the temperature on the weighing scheme?**

Hint: As a starting point, think about what happens to the softmax function as $\alpha \to 0$ (to make it even easier to think about, consider applying the softmax to two fixed values $a, b$ with $a > b$ as you do this).

⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠
The temperature controls how much the training process prioritizes high-return trajectories for the weights. For very small values of the temperature (alpha), nearing zero, the softmax function outputs a distribution that puts almost all of the weight on the highest-return trajectory and ignores all the other trajectories. In this case, the policy will only imitate the best trajectory but could overfit and lose generalization. For very large values of the temperature (alpha), the softmax function becomes a uniform distribution, giving each trajectory equal weight irregardless of the return value. The policy here will behave like standard behavioral cloning.

**Short Answer: One could consider a version of this reweighing scheme where we remove the softmax and simply define the weight for $\tau_i$ as $R(\tau_i)$. What are the potential pitfalls of such a scheme?**

Hint: Think about the values the reward function could take in all kinds of environments.

⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠

One potential pitfall of this scheme is that if there are negative returns the weights would then be negative. This could break the loss function and make the directions of the weights all different. Another potential pitfall is that this could lead outliers with really high returns to dominate the learning which would cause the model to overfit to the single trajectory.

### (d) Filtering Strategy II: Truncated Future Return???

Another spark of insight just hit Flappy Jr.: rather than just defining weights at the level of trajectories, he can also weight each datapoint separately! Floppy's flying motions aren't that sloppy all the time after all, just like how a broken clock is correct twice a day. For any datapoint $(s, a)$ and its reward $r_0$, he looks at the reward $(r_1, \dots, r_{T - 1})$ of the $T - 1$ following timesteps in the same trajectory\* and computes

$$R_{T}(s, a) := \sum_{t = 0}^{T - 1}\gamma^tr_t.$$

Then, if the computed values are $R_1, \dots, R_N$ for all points in the dataset, then the weight $w_i$ of the $i^{\text{th}}$ point is given by

$$w_i = N\cdot \text{Softmax}_i\left[\frac{1}{\alpha}R_1\ , \frac{1}{\alpha}R_2\ , \dots\ , \frac{1}{\alpha}R_N\right],$$

where $\alpha$ is the temperature hyperparameter (as in the previous part). Note that there are now two hyperparameters, the truncation horizon $T$, and the temperature $\alpha$.

\* **If there are fewer than $T - 1$ timesteps after $(s, a)$ in the trajectory that $(s,a)$ belongs to, the remaining timestep rewards in the sum are set to $0$.**

**Implement the function which computes the weights as defined above in the cell below, and performed filtered behavioral cloning with the computed weights.** Feel free to tune the temperature as necessary. How does the trained policy compare to that of the earlier policies in Problems 1(d) and 2(a)?
"""

from scipy.special import softmax

def truncated_future_return_weights(dataset, truncation_horizon, temp, discount) -> np.ndarray:
  """Computes an array of weights for each point in the provided dataset according to the truncated future return reweighing scheme.

  Args:
    dataset: Input dataset.
    truncation_horizon: How many timesteps to consider for computing future return.
    temp: Temperature used in softmax.
    discount: Discount used to compute return.

  Returns:
    An array of weights for each BC datapoint in the dataset.
  """
  pass
  rewards = data['rewards']
  is_rollout_start = data['is_rollout_start']

  trajectory_starts = np.where(is_rollout_start)[0].astype(int) t
  trajectory_starts = np.append(trajectory_starts, len(rewards))

  truncated_returns = np.zeros(len(rewards))

  # compute truncated returns for each timestep in the dataset
  truncated_returns = np.zeros(len(rewards))

  for i in range(len(trajectory_starts) - 1):
      start = trajectory_starts[i]
      end = trajectory_starts[i + 1]

      for t in range(start, end):
          future_rewards = rewards[t:min(t + truncation_horizon, end)]

          truncated_return = 0
          for j in range(len(future_rewards)):
              truncated_return += future_rewards[j] * (discount ** j)

          truncated_returns[t] = truncated_return


  softmax_weights = softmax(truncated_returns / temp)

  weights = softmax_weights * len(truncated_returns)

  return weights

### YOUR CODE HERE: RUN FILTERED BC BELOW!
pass
temperature = 0.5
discount = 0.999
truncation_horizon = 20

weights = truncated_future_return_weights(dataset, temperature, discount, truncation_horizon)

input_dim = observations.shape[1] # number of features
n_actions = len(torch.unique(actions))  # number of actions
policy = DiscretePolicy(input_dim, n_actions)

# train policy
trained_policy = train_policy_by_filtered_bc(
    policy=policy,
    dataset=dataset,
    weights=weights,
    n_steps=50,
    batch_size=64
)

env = flappy_bird_gym.make("FlappyBird-v0")

# define a callable policy for evaluating the training
class TrainedPolicy:
    def __init__(self, policy):
        self.policy = policy
    def __call__(self, state):
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        return self.policy(state_tensor)

# evaluate trained policy
discount = 0.999
trained_returns = [evaluate_policy(env, TrainedPolicy(trained_policy), discount) for _ in range(50)]
mean_trained_return = np.mean(trained_returns)

# evaluate random policy
random_returns = [evaluate_policy(env, RandomPolicy(), discount) for _ in range(50)]
mean_random_return = np.mean(random_returns)

print(f"Mean return (Trained policy): {mean_trained_return}")
print(f"Mean return (Random policy): {mean_random_return}")

"""**Short Answer: Let us explore the effect of the hyperparameter $T$ on the weighing scheme. Give a succinct description of the weights when $T = 1$. Do you expect this to work well in general? Why or why not?**

⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠
When T = 1, the weight for each state action pair only depends on the immediate reward and no future rewards. Thus, when computing softmax only the rewards at that current time are considered. I would expect this to not work that well in general becuase then the long term consequences of actions are not considered. This could also lead the model to overfit to short-term rewards.

**Short Answer: Can you think of a reason why one can set $T$ relatively small in Flappy Bird and still obtain decent performance?**

Hint: Think about the structure of the problem the agent is trying to solve.

⚠⚠⚠ **YOUR ANSWER HERE** ⚠⚠⚠
It's probably ok to set a relatively small T in Flappy Bird because in order to not die, flappy primarily needs to focus on passing through the immediate pipe. The game is also repetitive, with the future pipes being the same as the current pipes, just in different locations, so long term consequences aren't that important since they are basically the same as the current ones. In order to survive one simply has to focus on one pipe at a time so T can be set quite small.

### (e) Optional: Other Reweighing Schemes???

Feel free to try implementing other reweighing schemes here! Options to try:

1. Only training on the top $q\%$ of trajectories ranked by return.
2. Using the future return within the trajectory from the current timestep.
"""

def my_reweighing_scheme(dataset, *args, **kwargs) -> np.ndarray:
  """Computes an array of weights for each point in the provided dataset according to the truncated future return reweighing scheme.

  Args:
    dataset: Input dataset.
    *args: Replace with your scheme's hyperparameters.
    **kwargs: Replace with your scheme's hyperparameters.

  Returns:
    An array of weights for each BC datapoint in the dataset.
  """
  ### YOUR CODE HERE!
  pass


### YOUR CODE HERE: RUN FILTERED BC BELOW!